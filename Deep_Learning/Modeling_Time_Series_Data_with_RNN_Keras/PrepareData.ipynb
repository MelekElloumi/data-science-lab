{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "# configure notebook to display plots\n",
    "%matplotlib inline\n",
    "\n",
    "# set up user paths\n",
    "data_dir = '/dli/task/data/hx_series'\n",
    "csv_dir = '/dli/task/csv'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Fetch the DataFrame's loaded in the problem setup\n",
    "X_train=pd.read_pickle('X_train.pkl')\n",
    "y_train=pd.read_pickle('y_train.pkl')\n",
    "X_valid=pd.read_pickle('X_valid.pkl')\n",
    "y_valid=pd.read_pickle('y_valid.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create file path for csv file with metadata about variables\n",
    "metadata = os.path.join(csv_dir, 'ehr_features.csv')\n",
    "\n",
    "# read in variables from csv file (using pandas) since each varable there is tagged with a category\n",
    "variables = pd.read_csv(metadata, index_col=0)\n",
    "\n",
    "# next, select only variables of a particular category for normalization\n",
    "normvars = variables[variables['type'].isin(['Interventions', 'Labs', 'Vitals'])]\n",
    "\n",
    "# finally, iterate over each variable in both training and validation data\n",
    "for vId, dat in normvars.iterrows():\n",
    "\n",
    "    X_train[vId] = X_train[vId] - dat['mean']\n",
    "    X_valid[vId] = X_valid[vId] - dat['mean']\n",
    "    X_train[vId] = X_train[vId] / (dat['std'] + 1e-12)\n",
    "    X_valid[vId] = X_valid[vId] / (dat['std'] + 1e-12)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# first select variables which will be filled in\n",
    "fillvars = variables[variables['type'].isin(['Vitals', 'Labs'])].index\n",
    "\n",
    "# next forward fill any missing values with more recently observed value\n",
    "X_train[fillvars] = X_train.groupby(level=0)[fillvars].ffill()\n",
    "X_valid[fillvars] = X_valid.groupby(level=0)[fillvars].ffill()\n",
    "\n",
    "# finally, fill in any still missing values with 0 (i.e. values that could not be filled forward)\n",
    "X_train.fillna(value=0, inplace=True)\n",
    "X_valid.fillna(value=0, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "# max number of sequence length\n",
    "maxlen = 500\n",
    "\n",
    "# get a list of unique patient encounter IDs\n",
    "teId = X_train.index.levels[0]\n",
    "veId = X_valid.index.levels[0]\n",
    "\n",
    "# pad every patient sequence with 0s to be the same length,\n",
    "# then transforms the list of sequences to one numpy array\n",
    "# this is for efficient minibatching and GPU computations\n",
    "X_train = [X_train.loc[patient].values for patient in teId]\n",
    "y_train = [y_train.loc[patient].values for patient in teId]\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, dtype='float32', maxlen=maxlen, padding='post', truncating='post')\n",
    "y_train = sequence.pad_sequences(y_train, dtype='float32', maxlen=maxlen, padding='post', truncating='post')\n",
    "\n",
    "# repeat for the validation data\n",
    "\n",
    "X_valid = [X_valid.loc[patient].values for patient in veId]\n",
    "y_valid = [y_valid.loc[patient].values for patient in veId]\n",
    "\n",
    "X_valid = sequence.pad_sequences(X_valid, dtype='float32', maxlen=maxlen, padding='post', truncating='post')\n",
    "y_valid = sequence.pad_sequences(y_valid, dtype='float32', maxlen=maxlen, padding='post', truncating='post')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# figure out how many encounters we have\n",
    "numencnt = X_train.shape[0]\n",
    "\n",
    "# choose a random patient encounter to plot\n",
    "ix = random.randint(0,5000) #Try a few different index values between 0 and 4999\n",
    "print('ix = {}'.format(ix))\n",
    "\n",
    "# plot a matrix of observation values\n",
    "plt.title(\"Patient Encounter Matrix\")\n",
    "plt.pcolor(np.transpose(X_train[ix,:,:]))\n",
    "plt.ylabel(\"variable\")\n",
    "plt.xlabel(\"time/observation\")\n",
    "plt.ylim(0,265)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save the prepared numpy arrays for use in other notebooks\n",
    "np.save('X_train_prepared.npy',X_train,allow_pickle=False)\n",
    "np.save('y_train_prepared.npy',y_train,allow_pickle=False)\n",
    "np.save('X_valid_prepared.npy',X_valid,allow_pickle=False)\n",
    "np.save('y_valid_prepared.npy',y_valid,allow_pickle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}